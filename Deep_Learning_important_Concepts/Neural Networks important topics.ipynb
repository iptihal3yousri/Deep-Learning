{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7dfbb84",
   "metadata": {},
   "source": [
    "<h1>TASK_1 : All Types of Activation Functions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbe3d9",
   "metadata": {},
   "source": [
    "<h4>3 Types of Neural Networks Activation Functions</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5176e5",
   "metadata": {},
   "source": [
    ">1.   Binary Step Function\n",
    "2.   Linear Activation Function\n",
    "3.   Non-Linear Activation Functions\n",
    ">>1.    Swish\n",
    "  2.    ReLU Function\n",
    "  3.    Softmax Function\n",
    "  4.    Leaky ReLU Function\n",
    "  5.    Parametric ReLU Function\n",
    "  6.    Gaussian Error Linear Unit (GELU)\n",
    "  7.    Tanh Function (Hyperbolic Tangent)\n",
    "  8.    Scaled Exponential Linear Unit (SELU)\n",
    "  9.    Sigmoid / Logistic Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca595c",
   "metadata": {},
   "source": [
    " a few rules for choosing the activation function for your output layer based on the type of prediction problem that you are solving:\n",
    ">1.    Regression - Linear Activation Function\n",
    "2.    Binary Classification—Sigmoid/Logistic Activation Function\n",
    "3.    Multiclass Classification—Softmax\n",
    "4.    Multilabel Classification—Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b293993",
   "metadata": {},
   "source": [
    "And here are a few other guidelines to help you out.\n",
    "\n",
    ">1.    ReLU activation function should only be used in the hidden layers.\n",
    "2.    Sigmoid/Logistic and Tanh functions should not be used in hidden layers as they make the model more susceptible to problems during training (due to vanishing gradients).\n",
    "3.    Swish function is used in neural networks having a depth greater than 40 layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7bcb44",
   "metadata": {},
   "source": [
    "<h1>Task_2 : When to use Softmax and sigmoid</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1be0f",
   "metadata": {},
   "source": [
    ">1.    We use Softmax with Multiclass Classification problems\n",
    "2.    We use Sigmoid with Multilabel Classification problems\n",
    "\n",
    "in other word: \n",
    "Sigmoid is used for binary classification methods where we only have 2 classes, while SoftMax applies to multiclass problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c63d2",
   "metadata": {},
   "source": [
    "<h1>TASK_3 : Types of layers in Tensorflow(Usage of each Layer)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3553a9c",
   "metadata": {},
   "source": [
    ">1.   Dense :      simple layer of neurons in which each neuron receives input from all the neurons of previous layer\n",
    "2.   Flatten :     merges all visible layers into a single background layer\n",
    "3.   Drop out :    used in the construction of neural networks to prevent overfitting\n",
    "4.   Conv2D :\tComputes a 2-D convolution\n",
    "5.   Conv2DBackpropInput :\tReorganizes data from a batch into spatial data chunks\n",
    "6.   MaxPool :\tMostly on input, MaxPool performs maximum pooling.\n",
    "7.   AvgPool :\tAverage pooling is given to the input data.\n",
    "8.   Global Pooling : a pooling operation designed to replace fully connected layers in classical CNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1780d5",
   "metadata": {},
   "source": [
    "<h1>TASK_4 : What is the difference between batch size and steps per epoch</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e985ca5",
   "metadata": {},
   "source": [
    ">1.    A batch is a subset of the training data used to update the model's weights\n",
    "2.    The number of steps defines how many times the model goes through the training data.\n",
    "\n",
    "\n",
    "On the other hand, an epoch is defined as one complete pass through the entire training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cac33e",
   "metadata": {},
   "source": [
    "<h1>TASK_5 : Learning Rate</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b855fb",
   "metadata": {},
   "source": [
    ">1.    The learning rate, denoted by the symbol α, is a hyper-parameter used to govern the pace at which an algorithm updates or learns the values of a parameter estimate\n",
    "2.    A traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem\n",
    "3.    The range of values to consider for the learning rate is less than 1.0 and greater than 10^-6."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
